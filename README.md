

书生·浦语大模型的发展历程：浦语大模型经过将近一年的发展迭代，已经发布了InternLM22.浦语大模型可以根据自己的资源的情况，选择轻量模型和全量模型进行部署
![image](https://github.com/rudykon/InternLM/assets/15075498/fbea8743-b990-4dc9-b8f4-124998ec8883)# InternLM

浦语大模型2.0面向不同的使用需求，每个规格包含三个模型版本
![image](https://github.com/rudykon/InternLM/assets/15075498/b9c49be0-f3f7-468d-aad8-339d9504129d)

书生·浦语2.0（InternLM2）的主要亮点:超长上下文、综合性能全面提升、优秀的对话和创作体验、工具调用能力整体升级、突出的数理能力和实用的数据分析功能。
![image](https://github.com/rudykon/InternLM/assets/15075498/be868e37-ee49-4635-b75a-9ffdf677f2ac)

从模型到应用典型流程
![image](https://github.com/rudykon/InternLM/assets/15075498/92a97771-3961-4a95-b9de-ffdb6c46fc06)

浦语大模型拥有优秀的开发工具箱或者开放平台，方便用户对大模型进行二次开发
![image](https://github.com/rudykon/InternLM/assets/15075498/6e4b8501-79d6-4298-91f7-fc38ebc48f47)


浦语大模型拥有CompassKit大模型评测全栈工具链



InternLM2 技术报告阅读笔记摘要：以ChatGPT 和 GPT-4 为代表的大型语言模型 (LLM) 的出现引发了人们关于人工智能 (AGI) 的广泛讨论。然而，在开源模型中复制这样的成就颇具挑战性。本文介绍了 InternLM2，一个开源的大语言模型，它通过创新的预训练和微调技术在 6 个维度和 30 个基准、长上下文建模和开放式主观评价的综合评估中脱颖而出，表现优越。InternLM2 的预训练过程非常详细，着重提到了各种数据类型的制备，包括文本、代码和长上下文数据。InternLM2 有效地捕获长期依赖关系，最初在预训练和微调阶段提升到 32k 个标记之前在 4k 个标记上进行训练，在 200k“Needle-in-a-Haystack”测试中表现出卓越的性能。InternLM2 使用有监督的微调 (SFT) 进一步调整，以及条件性在线强化学习从人类反馈 (COOL RLHF) 策略的在线强化学习，该策略解决了相互冲突的人类偏好和奖励黑客。通过在不同的训练阶段和模型大小中发布 InternLM2 模型，我们向社区提供有关模型演变的见解。InternLM2架构：InternEvo框架：InternLM2采用的InternEvo训练框架，通过先进的并行处理技术和内存优化策略，实现了高效的大规模训练，特别是在处理长序列数据时展现出了其独特优势。长文本建设能力：InternLM2在长文本处理方面展示了显著的性能提升。通过特别设计的长上下文训练阶段，模型能够理解和生成远超过之前模型限制的文本长度，为处理复杂文档、编写详尽报告等应用开辟了新的可能性。创新与训练过程：InternLM2通过一个精心设计的预训练流程来提升模型性能，特别是引入了长上下文训练和特定能力增强训练。这一策略不仅使模型能够处理更长的文本，还针对特定任务提升了模型的专业能力，如编程、推理和语言理解，这在之前的模型中不是特别常见。高效的基础设施建设：InternLM2背后的InternEvo框架优化了数据处理、模型训练和资源利用效率，尤其是在分布式GPU环境中。这不仅提高了训练速度，还保证了在处理大规模数据集时的高效性，为未来模型的发展提供了可扩展的基础。综合性能显著提高：通过在多个标准评估任务上的表现，InternLM2证明了其在理解、推理、语言生成等多个方面的能力显著超越了现有的大型语言模型。这种全面的性能提升，特别是在专业领域如编程和数学问题解答上的突出表现，体现了其深厚的知识储备和逻辑处理能力。
