![image](https://github.com/rudykon/InternLM/assets/15075498/523e9fa0-f2e1-4ba3-8312-616a8a02e4d2)

书生·浦语大模型的发展历程
![image](https://github.com/rudykon/InternLM/assets/15075498/fbea8743-b990-4dc9-b8f4-124998ec8883)# InternLM

InternLM2面向不同的使用需求，每个规格包含三个模型版本
![image](https://github.com/rudykon/InternLM/assets/15075498/b9c49be0-f3f7-468d-aad8-339d9504129d)

InternLM2的主要亮点:超长上下文、综合性能全面提升、优秀的对话和创作体验、工具调用能力整体升级、突出的数理能力和实用的数据分析功能。
![image](https://github.com/rudykon/InternLM/assets/15075498/be868e37-ee49-4635-b75a-9ffdf677f2ac)

从模型到应用典型流程
![image](https://github.com/rudykon/InternLM/assets/15075498/92a97771-3961-4a95-b9de-ffdb6c46fc06)

InternLM2拥有优秀的开发工具箱或者开放应用测试平台，方便用户对大模型进行二次开发
![image](https://github.com/rudykon/InternLM/assets/15075498/6e4b8501-79d6-4298-91f7-fc38ebc48f47)


InternLM2 技术报告阅读笔记摘要：

基础设施：模型训练使用高效的轻量级预训练框架InternEvo进行模型训练。语言模型架构选择了Transformer，InternLM2遵循LLaMA的结构设计原则，并作了改进。

预训练：
预训练数据准备了通用领域文本数据、编程语言相关数据以及长文本数据
![image](https://github.com/rudykon/InternLM/assets/15075498/59471e76-830a-4ec6-8b52-c83dde3195a7)
![image](https://github.com/rudykon/InternLM/assets/15075498/47e37d8e-7378-44d7-a448-f8a3cd41b23b)
预训练设置采用GPT-4的分词方法，因为其在压缩大量文本内容方面的高效性表现出色。
预训练过程分为三个阶段。第一阶段，使用不超过4k长度的预训练语料库。第二阶段，加入了50%不超过32k长度的预训练数据。第三阶段，引入了特定能力增强数据。



以ChatGPT 和 GPT-4 为代表的大型语言模型 (LLM) 的出现引发了人们关于人工智能 (AGI) 的广泛讨论。然而，在开源模型中复制这样的成就颇具挑战性。本文介绍了 InternLM2，一个开源的大语言模型，它通过创新的预训练和微调技术在 6 个维度和 30 个基准、长上下文建模和开放式主观评价的综合评估中脱颖而出，表现优越。InternLM2 的预训练过程非常详细，着重提到了各种数据类型的制备，包括文本、代码和长上下文数据。InternLM2 有效地捕获长期依赖关系，最初在预训练和微调阶段提升到 32k 个标记之前在 4k 个标记上进行训练，在 200k“Needle-in-a-Haystack”测试中表现出卓越的性能。InternLM2 使用有监督的微调 (SFT) 进一步调整，以及条件性在线强化学习从人类反馈 (COOL RLHF) 策略的在线强化学习，该策略解决了相互冲突的人类偏好和奖励黑客。通过在不同的训练阶段和模型大小中发布 InternLM2 模型，我们向社区提供有关模型演变的见解。InternLM2架构：InternEvo框架：InternLM2采用的InternEvo训练框架，通过先进的并行处理技术和内存优化策略，实现了高效的大规模训练，特别是在处理长序列数据时展现出了其独特优势。长文本建设能力：InternLM2在长文本处理方面展示了显著的性能提升。通过特别设计的长上下文训练阶段，模型能够理解和生成远超过之前模型限制的文本长度，为处理复杂文档、编写详尽报告等应用开辟了新的可能性。创新与训练过程：InternLM2通过一个精心设计的预训练流程来提升模型性能，特别是引入了长上下文训练和特定能力增强训练。这一策略不仅使模型能够处理更长的文本，还针对特定任务提升了模型的专业能力，如编程、推理和语言理解，这在之前的模型中不是特别常见。高效的基础设施建设：InternLM2背后的InternEvo框架优化了数据处理、模型训练和资源利用效率，尤其是在分布式GPU环境中。这不仅提高了训练速度，还保证了在处理大规模数据集时的高效性，为未来模型的发展提供了可扩展的基础。综合性能显著提高：通过在多个标准评估任务上的表现，InternLM2证明了其在理解、推理、语言生成等多个方面的能力显著超越了现有的大型语言模型。这种全面的性能提升，特别是在专业领域如编程和数学问题解答上的突出表现，体现了其深厚的知识储备和逻辑处理能力。
